{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1119447-20ae-402e-9e42-1414f2e973be",
   "metadata": {},
   "source": [
    "## Image load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c2143f58-214a-477f-b8ff-edf85009da30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "path = 'C:\\\\Users\\\\seojeongP\\\\Desktop\\\\2021\\\\영상처리와 딥러닝\\\\final_project\\\\images\\\\images'\n",
    "os.chdir(path)\n",
    "files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91a0339-bf54-4813-b53b-20b4bb4f8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "png_img = []\n",
    "jpg_img = []\n",
    "for file in files:\n",
    "    if '.jpg' in file: \n",
    "        f = cv2.imread(file, cv2.IMREAD_COLOR)\n",
    "        jpg_img.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b267ee9a-78f7-4bd6-b645-369074a58f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\seojeongP\\\\Desktop\\\\2021\\\\영상처리와 딥러닝\\\\final_project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e727a41-4ff4-46ad-b6e7-7fe42987ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 원본 이미지 픽셀 맞춰주기\n",
    "\n",
    "back = cv2.imread(\"./img/back.jpg\", cv2.IMREAD_COLOR)\n",
    "mask = cv2.imread(\"./img/mask2.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "if back is None: raise Exception(\"back 읽기 오류\")\n",
    "    \n",
    "if mask is None: raise Exception(\"mask 읽기 오류\")\n",
    "\n",
    "mask = cv2.resize(mask, dsize=(147,147), interpolation=cv2.INTER_AREA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984148e8-d441-427a-bf0b-e0e6f68aa208",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 147",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\SEOJEO~1\\AppData\\Local\\Temp/ipykernel_8440/2631256776.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mpart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mback2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m243\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m390\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m147\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m294\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mfaceimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfaceimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m58\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m56\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m147\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m147\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mINTER_AREA\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mpart\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfaceimg\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mback2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m243\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m390\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m147\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m294\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpart\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 0 but corresponding boolean dimension is 147"
     ]
    }
   ],
   "source": [
    "for i, faceimg in enumerate(jpg_img):\n",
    "    back2 = back\n",
    "    part = back2[243:390, 147:294]\n",
    "    faceimg = cv2.resize(faceimg[8:58, 10:56], dsize=(147,147), interpolation=cv2.INTER_AREA)\n",
    "    part[mask > 0] = faceimg[mask > 0]\n",
    "    back2[243:390, 147:294] = part\n",
    "    if(i<10): \n",
    "        ab = '00{0}'.format(i)\n",
    "    elif(i<100): \n",
    "        ab='0{0}'.format(i)\n",
    "    else: \n",
    "        ab=i\n",
    "    cv2.imwrite('./back/back/{0}.jpg'.format(ab), back2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92626383-0126-4136-bec1-5d61acc36516",
   "metadata": {},
   "source": [
    "## bold image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "6e43ce5e-4495-411d-821b-0ef78574eb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('C:\\\\Users\\\\seojeongP\\\\Desktop\\\\2021\\\\영상처리와 딥러닝\\\\final_project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dcc4713a-50d0-4103-b098-e91004b293d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "origin = cv2.imread(\"./img/mannequiin.jpg\", cv2.IMREAD_COLOR)\n",
    "mask = cv2.imread(\"./img/mask2.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "if origin is None: raise Exception(\"영상파일 읽기 오류\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2e02e901-8bbe-4816-a51e-7acff6c62a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, faceimg in enumerate(jpg_img):\n",
    "    origin2 = origin\n",
    "    part = origin2[132:311, 41:220]\n",
    "    faceimg = cv2.resize(faceimg[8:58, 10:56], dsize=(179,179), interpolation=cv2.INTER_AREA)\n",
    "    part[mask > 0] = faceimg[mask > 0]\n",
    "    origin2[132:311, 41:220] = part\n",
    "    if(i<10): \n",
    "        ab = '00{0}'.format(i)\n",
    "    elif(i<100): \n",
    "        ab='0{0}'.format(i)\n",
    "    else: \n",
    "        ab=i\n",
    "    cv2.imwrite('./bold_data/bold_data/{0}.jpg'.format(ab), origin2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464d839d-7c43-43cf-bcb1-b736743e9760",
   "metadata": {},
   "source": [
    "## fomad hair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "086748c0-ce61-4cfc-a5dd-0caa79e91bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fomad = cv2.imread(\"./img/fomad2.jpg\", cv2.IMREAD_COLOR)\n",
    "fomad = cv2.resize(fomad, dsize=(455, 455))\n",
    "mask = cv2.resize(mask, dsize=(147,147), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "if fomad is None: raise Exception(\"영상파일 읽기 오류\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "eafd0dd7-93c5-4bdd-b943-a23efdd4b832",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, faceimg in enumerate(jpg_img):\n",
    "    fomad2 = fomad\n",
    "#     part = fomad2[191:381, 133:323]\n",
    "    part = fomad2[243:390, 147:294]\n",
    "    faceimg = cv2.resize(faceimg[8:58, 10:56], dsize=(147,147), interpolation=cv2.INTER_AREA)\n",
    "    part[mask > 0] = faceimg[mask > 0]\n",
    "    fomad2[243:390, 147:294] = part\n",
    "    if(i<10): \n",
    "        ab = '00{0}'.format(i)\n",
    "    elif(i<100): \n",
    "        ab='0{0}'.format(i)\n",
    "    else: \n",
    "        ab=i\n",
    "    cv2.imwrite('./fomad_data/fomad_data/{0}.jpg'.format(ab), fomad2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dde8776-46ea-432a-8716-296f6324e11b",
   "metadata": {},
   "source": [
    "## apro hair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3c7f8f3e-bc00-4b86-ad7d-8d44fb51d315",
   "metadata": {},
   "outputs": [],
   "source": [
    "apro = cv2.imread(\"./img/apro2.jpg\", cv2.IMREAD_COLOR)\n",
    "apro = cv2.resize(apro, dsize=(455, 455))\n",
    "mask = cv2.resize(mask, dsize=(147,147), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "if fomad is None: raise Exception(\"영상파일 읽기 오류\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7045a726-d0e6-4f25-9779-237cb67a7897",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, faceimg in enumerate(jpg_img):\n",
    "    apro2 = apro\n",
    "    part = apro2[243:390, 147:294]\n",
    "    faceimg = cv2.resize(faceimg[8:58, 10:56], dsize=(147,147), interpolation=cv2.INTER_AREA)\n",
    "    part[mask > 0] = faceimg[mask > 0]\n",
    "    apro2[243:390, 147:294] = part\n",
    "    if(i<10): \n",
    "        ab = '00{0}'.format(i)\n",
    "    elif(i<100): \n",
    "        ab='0{0}'.format(i)\n",
    "    else: \n",
    "        ab=i\n",
    "    cv2.imwrite('./apro_data/apro_data/{0}.jpg'.format(ab), apro2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b2448d-55bb-443b-8ced-fe297bc72fa9",
   "metadata": {},
   "source": [
    "## scratch hair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "1ccf84b9-0c36-470e-b9f3-2456d54c9f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "scratch = cv2.imread(\"./img/scratch3.jpg\", cv2.IMREAD_COLOR)\n",
    "\n",
    "scratch = cv2.resize(scratch, dsize=(455, 455))\n",
    "mask = cv2.resize(mask, dsize=(147,147), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "if scratch is None: raise Exception(\"영상파일 읽기 오류\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a42b8913-4531-4149-b19f-32e39acbe083",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, faceimg in enumerate(jpg_img):\n",
    "    scratch2 = scratch\n",
    "    part = scratch2[243:390, 147:294]\n",
    "    faceimg = cv2.resize(faceimg[8:58, 10:56], dsize=(147,147), interpolation=cv2.INTER_AREA)\n",
    "    part[mask > 0] = faceimg[mask > 0]\n",
    "    scratch2[243:390, 147:294] = part\n",
    "    if(i<10): \n",
    "        ab = '00{0}'.format(i)\n",
    "    elif(i<100): \n",
    "        ab='0{0}'.format(i)\n",
    "    else: \n",
    "        ab=i\n",
    "    cv2.imwrite('./scratch_data/scratch_data/{0}.jpg'.format(ab), scratch2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b30c8b9-e8a7-4b1c-9029-b91c816b1216",
   "metadata": {},
   "source": [
    "## short hair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "5c2ab411-7171-4fb0-b299-2a67e0535e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "short = cv2.imread(\"./img/short.jpg\", cv2.IMREAD_COLOR)\n",
    "short = cv2.resize(short, dsize=(455, 455))\n",
    "mask = cv2.resize(mask, dsize=(147,147), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "if short is None: raise Exception(\"영상파일 읽기 오류\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "86a542ae-1987-4974-8cd8-00b806ed1695",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, faceimg in enumerate(jpg_img):\n",
    "    short2 = short\n",
    "    part = short2[243:390, 147:294]\n",
    "    faceimg = cv2.resize(faceimg[8:58, 10:56], dsize=(147,147), interpolation=cv2.INTER_AREA)\n",
    "    part[mask > 0] = faceimg[mask > 0]\n",
    "    short2[243:390, 147:294] = part\n",
    "    if(i<10): \n",
    "        ab = '00{0}'.format(i)\n",
    "    elif(i<100): \n",
    "        ab='0{0}'.format(i)\n",
    "    else: \n",
    "        ab=i\n",
    "    cv2.imwrite('./short_data/short_data/{0}.jpg'.format(ab), short2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4095663-05a4-4502-bf1f-5ce6431e8959",
   "metadata": {},
   "source": [
    "## AutoEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bd62134-9164-4702-9235-410e13f62846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.init as init\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea62007d-ef6a-439a-a445-0ce6dd9fb694",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "learning_rate = 0.0002\n",
    "num_epoch = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d7db907c-d2ed-4a89-a0f0-1d0c90b0fff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './bold_data/*.jpg'\n",
    "\n",
    "file_list = glob(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48767ab2-ef97-4509-9bae-65985b32d853",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8a8a4d3-62b0-47c6-9f39-2b14a1809a8a",
   "metadata": {},
   "source": [
    "### DataLoader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e18a9d-b0cf-4f81-8986-b779393b1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class Classification_Dataset(Dataset):\n",
    "    def __init__(self, csv, mode, transform=None):\n",
    "        self.csv = csv.reset_index(drop=True)  # random으로 섞인 데이터의 인덱스를 reset 시켜서 다시 부여한다.\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.csv.shape[0]  # csv 파일의 행 개수 == 데이터 개수\n",
    "\n",
    "    def __getitem__(self, index):   \n",
    "        row = self.csv.iloc[index]                       # 주어진 index에 대한 데이터 뽑아오기\n",
    "        image = Image.open(row.file_path).convert('RGB') # 파일경로로 부터 이미지를 읽고 rgb로 변환하기\n",
    "        target = torch.tensor(self.csv.iloc[index].target).long()\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image) # 이미지에 transform 적용하기\n",
    "\n",
    "        return image, target  # 이미지와 target return하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df738662-2cbb-4615-80d3-52adaaa98d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "dataset_train = Classification_Dataset(df_train, 'train', transform=transforms.ToTensor())\n",
    "\n",
    "def get_transforms(image_size):\n",
    "\n",
    "    transforms_train = transforms.Compose([\n",
    "                                       transforms.RandomRotation(30),\n",
    "                                       transforms.RandomResizedCrop(image_size),\n",
    "                                       transforms.RandomHorizontalFlip(),\n",
    "                                       transforms.ToTensor()])\n",
    "    \n",
    "    transforms_test = transforms.Compose([transforms.Resize(image_size+30),\n",
    "                                      transforms.CenterCrop(image_size),\n",
    "                                      transforms.ToTensor()])\n",
    "    \n",
    "    \n",
    "    return transforms_train, transforms_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c91e821-38f1-4cfa-a7b0-2a0fb34fa1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 트랜스폼 가져오기\n",
    "transforms_train, transforms_test= get_transforms(224)\n",
    "\n",
    "# dataset class 객체 만들기\n",
    "dataset_train = Classification_Dataset(df_train, 'train', transform=transforms_train)\n",
    "dataset_test = Classification_Dataset(df_test, 'valid', transform=transforms_test)\n",
    "\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset_train, batch_size=20, sampler=RandomSampler(dataset_train), num_workers=0)\n",
    "test_loader = torch.utils.data.DataLoader(dataset_test, batch_size=20, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d7e00e-0091-4967-a59b-787c5deb2c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, targets in train_loader:\n",
    "    print(f'[batch, channel, ...] : {images.shape}')\n",
    "    print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8306a1eb-6648-4b41-ba8a-9a7cf86cb8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, targets in test_loader:\n",
    "    print(f'[batch, channel, ...] : {images.shape}')\n",
    "    print(targets.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9271e46d-bba6-492b-b463-e0633f658a3a",
   "metadata": {},
   "source": [
    "### Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb669d-68c1-425a-a5e9-d654ffff7b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder,self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.Conv2d(3, 16, 3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(16),\n",
    "                        nn.Conv2d(16, 32, 3, padding=1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(32),\n",
    "                        nn.MaxPool2d(2, 2)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.BatchNorm2d(32),\n",
    "                        nn.MaxPool2d(2, 2),\n",
    "                        nn.Conv2d(32, 32, 3, padding=1),\n",
    "                        nn.ReLU()\n",
    "        )\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(batch_size, -1)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d5c19d-b154-4f6a-ab99-1279e610f5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(32, 32, 3,2, 1, 1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(32),\n",
    "                        nn.Conv2d(32, 16, 3, 1, 1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(16)\n",
    "        )\n",
    "        \n",
    "        self.layer2 = nn.Sequential(\n",
    "                        nn.ConvTranspose2d(16, 16, 3, 1, 1),\n",
    "                        nn.ReLU(),\n",
    "                        nn.BatchNorm2d(16),\n",
    "                        nn.ConvTranspose2d(16, 3, 3, 2, 1,1),\n",
    "                        nn.ReLU()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        out = x.view(batch_size,32,56,56)\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc40b15-78db-4d3d-8fa0-44e4ad8b02df",
   "metadata": {},
   "source": [
    "### model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd95cfc-afba-469e-b750-e5681b120593",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "encoder = Encoder().to(device)\n",
    "decoder = Decoder().to(device)\n",
    "\n",
    "# 인코더 디코더의 파라미터를 동시에 학습시키기 위해 이를 묶는 방법입니다.\n",
    "parameters = list(encoder.parameters())+ list(decoder.parameters())\n",
    "\n",
    "loss_func = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "962974c2-1ac0-4bc4-ba84-5a77a1d15d42",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd98101c-5c4b-4203-9de4-f5c246b44dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델을 불러오는 방법입니다.\n",
    "# 크게 두가지 방법이 있는데 여기 사용된 방법은 좀 단순한 방법입니다.\n",
    "# https://pytorch.org/tutorials/beginner/saving_loading_models.html\n",
    "\n",
    "try:\n",
    "    encoder, decoder = torch.load('./model/conv_autoencoder.pkl')\n",
    "    print(\"\\n--------model restored--------\\n\")\n",
    "except:\n",
    "    print(\"\\n--------model not restored--------\\n\")\n",
    "    pass\n",
    "\n",
    "for i in range(num_epoch):\n",
    "    for j,[image,label] in enumerate(train_loader):\n",
    "      \n",
    "        optimizer.zero_grad()\n",
    "        image = image.to(device)\n",
    "        \n",
    "        output = encoder(image)\n",
    "        output = decoder(output)\n",
    "        \n",
    "        loss = loss_func(output, image)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_1.1",
   "language": "python",
   "name": "pytorch_1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
